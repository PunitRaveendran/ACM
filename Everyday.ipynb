{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWRBYJHkMJicBUaUZXzb3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PunitRaveendran/ACM/blob/main/Everyday.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "df = pd.read_csv(\"emotion_dataset_raw.csv\")\n",
        "df['Text'] = df['Text'].astype(str)\n",
        "\n",
        "# Label encode\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['Emotion'])\n",
        "\n",
        "def tokenize(text): return text.lower().split()\n",
        "\n",
        "# Build vocab from tokens\n",
        "tokens = [token for text in df['Text'] for token in tokenize(text)]\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "vocab.update({w: i + 2 for i, (w, _) in enumerate(Counter(tokens).most_common(8000))})\n",
        "\n",
        "# Encoding text\n",
        "def encode(text): return [vocab.get(w, 1) for w in tokenize(text)]\n",
        "df['input_ids'] = df['Text'].apply(encode)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['input_ids'], df['label'], test_size=0.2, stratify=df['label'])\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = [torch.tensor(x) for x in texts]\n",
        "        self.labels = torch.tensor(labels.values)\n",
        "\n",
        "    def __getitem__(self, idx): return self.texts[idx], self.labels[idx]\n",
        "    def __len__(self): return len(self.labels)\n",
        "\n",
        "def pad_collate(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    padded = pad_sequence(texts, batch_first=True)\n",
        "    return padded, torch.tensor(labels)\n",
        "\n",
        "train_dl = DataLoader(EmotionDataset(X_train, y_train), batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
        "test_dl  = DataLoader(EmotionDataset(X_test, y_test), batch_size=64, collate_fn=pad_collate)\n",
        "\n",
        "# Model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        return self.fc(h[-1])\n",
        "\n",
        "model = LSTM(len(vocab), 100, 128, len(le.classes_)).to(\"cpu\")\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    for X, y in train_dl:\n",
        "        opt.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    print(f\"Epoch {epoch+1} complete\")\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "all_preds, all_true = [], []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dl:\n",
        "        logits = model(X)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_true.extend(y.tolist())\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(all_true, all_preds))\n",
        "print(\"F1 Score:\", f1_score(all_true, all_preds, average='weighted'))\n",
        "\n",
        "# Sample Predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(10):\n",
        "    text_tokens = X_test.iloc[i]\n",
        "    text_tensor = torch.tensor(text_tokens).unsqueeze(0)\n",
        "    padded = pad_sequence([text_tensor.squeeze()], batch_first=True)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(padded)\n",
        "        pred_label = output.argmax(dim=1).item()\n",
        "\n",
        "    input_words = [word for word, idx in vocab.items() if idx in text_tokens][:15]\n",
        "    print(f\"Text: {' '.join(input_words)}...\")\n",
        "    print(f\"Actual: {le.inverse_transform([y_test.iloc[i]])[0]} | Predicted: {le.inverse_transform([pred_label])[0]}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh5smVTnpnso",
        "outputId": "3021d6aa-5d48-4287-f55c-ee92d676182e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete\n",
            "Epoch 2 complete\n",
            "Epoch 3 complete\n",
            "Accuracy: 0.42621066245150163\n",
            "F1 Score: 0.33658006756665154\n",
            "\n",
            "Sample Predictions:\n",
            "Text: <UNK> i to my in not be so they no were am being over things...\n",
            "Actual: anger | Predicted: anger\n",
            "\n",
            "Text: in you when on have your &amp; don't know even every you're scared it! light...\n",
            "Actual: fear | Predicted: fear\n",
            "\n",
            "Text: <UNK> i is it have so like no now looks bought nails skin nail practically...\n",
            "Actual: joy | Predicted: joy\n",
            "\n",
            "Text: <UNK> the to that you one really want thing person notice obvious...\n",
            "Actual: sadness | Predicted: anger\n",
            "\n",
            "Text: <UNK> a and in for two days dinner leaving reading drinking beer cooking...\n",
            "Actual: joy | Predicted: fear\n",
            "\n",
            "Text: i to and . , was that me had this time by very or think...\n",
            "Actual: sadness | Predicted: fear\n",
            "\n",
            "Text: the to and of that we are one our away twitter 5 follow giving win...\n",
            "Actual: surprise | Predicted: joy\n",
            "\n",
            "Text: <UNK> the . not by were miffed frantic stock currency...\n",
            "Actual: anger | Predicted: anger\n",
            "\n",
            "Text: <UNK> i a for have wait better day, hoping conditioning...\n",
            "Actual: joy | Predicted: joy\n",
            "\n",
            "Text: <UNK> after 6 faithful...\n",
            "Actual: sadness | Predicted: anger\n",
            "\n"
          ]
        }
      ]
    }
  ]
}