{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmQUShNfO2zWiBCvfo9d1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PunitRaveendran/ACM/blob/main/MAIN_ACM_TASK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL TASK"
      ],
      "metadata": {
        "id": "i-5v6BtiSe0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "2w5h4lVySizP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWuFi9xdEYE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1\n"
      ],
      "metadata": {
        "id": "JCWV2lJQ3ZKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3"
      ],
      "metadata": {
        "id": "WUoNQ0SAEbe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "def load_txt_file(file_path, num_lines_to_print=5):\n",
        "    \"\"\"\n",
        "    Reads a text file and attempts to parse it into a pandas DataFrame\n",
        "    with 'label' and 'text' columns. Prints the first few lines to help\n",
        "    understand the file format.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    print(f\"Reading file: {file_path}\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i < num_lines_to_print:\n",
        "                print(f\"Line {i+1}: {line.strip()}\")\n",
        "\n",
        "            # Attempt to parse the line - split by semicolon\n",
        "            parts = line.strip().split(';', maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                text, label = parts  # Correct order: text then label\n",
        "                data.append({'label': label, 'text': text})\n",
        "            # Optionally handle lines that don't match the expected format\n",
        "            # else:\n",
        "            #     print(f\"Skipping malformed line: {line.strip()}\")\n",
        "\n",
        "    if not data:\n",
        "        print(f\"No data parsed from {file_path}. Check file path and format.\")\n",
        "        return pd.DataFrame(columns=['label', 'text']) # Return empty df with columns\n",
        "    return pd.DataFrame(data)\n",
        "train_df = load_txt_file('/content/train.txt')\n",
        "val_df = load_txt_file('/content/val.txt')\n",
        "test_df = load_txt_file('/content/test.txt')\n",
        "\n",
        "# Preprocessing - Clean labels in each dataframe before combining or fitting LabelEncoder\n",
        "train_df['label'] = train_df['label'].str.lower().str.strip()\n",
        "val_df['label'] = val_df['label'].str.lower().str.strip()\n",
        "test_df['label'] = test_df['label'].str.lower().str.strip()\n",
        "\n",
        "# Print unique labels before filtering\n",
        "print(\"Unique labels in train_df before filtering:\", train_df['label'].unique())\n",
        "print(\"Unique labels in val_df before filtering:\", val_df['label'].unique())\n",
        "print(\"Unique labels in test_df before filtering:\", test_df['label'].unique())\n",
        "\n",
        "\n",
        "# Filter out unwanted labels from each dataframe\n",
        "train_df = train_df[train_df['label'].isin(['sadness', 'anger', 'love', 'joy'])]\n",
        "val_df = val_df[val_df['label'].isin(['sadness', 'anger', 'love', 'joy'])]\n",
        "test_df = test_df[test_df['label'].isin(['sadness', 'anger', 'love', 'joy'])]\n",
        "\n",
        "\n",
        "all_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\\\d+',' ',text)\n",
        "    return text.strip()\n",
        "\n",
        "all_df['text_clean'] = all_df['text'].apply(clean_text)\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Fit LabelEncoder on the labels that will be used for training, validation, and testing\n",
        "le.fit(all_df['label'])\n",
        "\n",
        "max_vocab = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(all_df['text_clean'])\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['text'].apply(clean_text)), maxlen=max_len)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['text'].apply(clean_text)), maxlen=max_len)\n",
        "X_val = pad_sequences(tokenizer.texts_to_sequences(val_df['text'].apply(clean_text)),maxlen=max_len)\n",
        "\n",
        "# Transform the labels in each dataframe\n",
        "y_train = le.transform(train_df['label'])\n",
        "y_test = le.transform(test_df['label'])\n",
        "y_val = le.transform(val_df['label'])\n",
        "\n",
        "# ---------- LSTM -----------\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(max_vocab, 64, input_length=max_len),\n",
        "    LSTM(64),\n",
        "    Dense(4, activation='softmax') # Changed from 3 to 4\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('F1 Score:', f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "# Sample prediction\n",
        "def predict_text(text):\n",
        "    seq = pad_sequences(tokenizer.texts_to_sequences([clean_text(text)]), maxlen=max_len)\n",
        "    pred = np.argmax(model.predict(seq), axis=1)[0]\n",
        "    return le.classes_[pred]\n",
        "\n",
        "print(predict_text(\"I am feeling great about my progress!\"))\n",
        "print(predict_text(\"I don't care about this.\"))\n",
        "print(predict_text(\"This is terrible and I give up.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIDhRUPcEgDB",
        "outputId": "1944fa4b-eae1-4ba2-b854-3bb37d673907"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file: /content/train.txt\n",
            "Line 1: i didnt feel humiliated;sadness\n",
            "Line 2: i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake;sadness\n",
            "Line 3: im grabbing a minute to post i feel greedy wrong;anger\n",
            "Line 4: i am ever feeling nostalgic about the fireplace i will know that it is still on the property;love\n",
            "Line 5: i am feeling grouchy;anger\n",
            "Reading file: /content/val.txt\n",
            "Line 1: im feeling quite sad and sorry for myself but ill snap out of it soon;sadness\n",
            "Line 2: i feel like i am still looking at a blank canvas blank pieces of paper;sadness\n",
            "Line 3: i feel like a faithful servant;love\n",
            "Line 4: i am just feeling cranky and blue;anger\n",
            "Line 5: i can have for a treat or if i am feeling festive;joy\n",
            "Reading file: /content/test.txt\n",
            "Line 1: im feeling rather rotten so im not very ambitious right now;sadness\n",
            "Line 2: im updating my blog because i feel shitty;sadness\n",
            "Line 3: i never make her separate from me because i don t ever want her to feel like i m ashamed with her;sadness\n",
            "Line 4: i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived;joy\n",
            "Line 5: i was feeling a little vain when i did this one;sadness\n",
            "Unique labels in train_df before filtering: ['sadness' 'anger' 'love' 'surprise' 'fear' 'joy']\n",
            "Unique labels in val_df before filtering: ['sadness' 'love' 'anger' 'joy' 'fear' 'surprise']\n",
            "Unique labels in test_df before filtering: ['sadness' 'joy' 'fear' 'anger' 'love' 'surprise']\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 55ms/step - accuracy: 0.4762 - loss: 1.1674 - val_accuracy: 0.8278 - val_loss: 0.4268\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 56ms/step - accuracy: 0.9085 - loss: 0.2584 - val_accuracy: 0.9373 - val_loss: 0.1855\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 58ms/step - accuracy: 0.9749 - loss: 0.0774 - val_accuracy: 0.9438 - val_loss: 0.1888\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 56ms/step - accuracy: 0.9833 - loss: 0.0507 - val_accuracy: 0.9420 - val_loss: 0.1950\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 57ms/step - accuracy: 0.9893 - loss: 0.0336 - val_accuracy: 0.9443 - val_loss: 0.1979\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
            "Accuracy: 0.9304093567251462\n",
            "F1 Score: 0.9080246818187756\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.91      0.93      0.92       275\n",
            "         joy       0.95      0.93      0.94       695\n",
            "        love       0.78      0.85      0.81       159\n",
            "     sadness       0.97      0.95      0.96       581\n",
            "\n",
            "    accuracy                           0.93      1710\n",
            "   macro avg       0.90      0.92      0.91      1710\n",
            "weighted avg       0.93      0.93      0.93      1710\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "joy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "love\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "sadness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88845484"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_txt_file(file_path, num_lines_to_print=5):\n",
        "    \"\"\"\n",
        "    Reads a text file and attempts to parse it into a pandas DataFrame\n",
        "    with 'label' and 'text' columns. Prints the first few lines to help\n",
        "    understand the file format.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    print(f\"Reading file: {file_path}\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i < num_lines_to_print:\n",
        "                print(f\"Line {i+1}: {line.strip()}\")\n",
        "\n",
        "            # Attempt to parse the line - split by semicolon\n",
        "            parts = line.strip().split(';', maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                text, label = parts  # Correct order: text then label\n",
        "                data.append({'label': label, 'text': text})\n",
        "            # Optionally handle lines that don't match the expected format\n",
        "            # else:\n",
        "            #     print(f\"Skipping malformed line: {line.strip()}\")\n",
        "\n",
        "    if not data:\n",
        "        print(f\"No data parsed from {file_path}. Check file path and format.\")\n",
        "        return pd.DataFrame(columns=['label', 'text']) # Return empty df with columns\n",
        "    return pd.DataFrame(data)"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}